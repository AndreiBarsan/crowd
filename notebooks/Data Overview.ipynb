{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Data Overview\n",
    "\n",
    "This notebook contains some general info regarding the data available for this project.\n",
    "\n",
    "The judgements used by this project are the NIST expert judgements ('stage1-dev') and the consensus labels ('stage2-dev') from [the 2011 TREC Crowdsourcing track](https://sites.google.com/site/treccrowd/2011).\n",
    "\n",
    "The actual document data used is, sadly, not publicly available (http://lemurproject.org/clueweb09/, ClueWeb09 dataset, T11Crowd subsection), but can be acquired by signing a non-commercial use agreement with the provider."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the data management code has been cannibalized from [Martin Davtyan's previous work on the subject](https://github.com/martinthenext/ir-crowd-thesis) (while at ETH Zurich)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This should be the root folder containing the different judgement datasets.\n",
    "DATA_ROOT = os.path.join(os.getenv(\"HOME\"), 'data')\n",
    "\n",
    "# This file contains exclusively the NIST expert judgements from 'stage1-dev'\n",
    "# 'cat'ed together into a single file (from all the teams, as well as the\n",
    "# common data).\n",
    "EXPERT_GROUND_TRUTH_FILE = os.path.join(DATA_ROOT, 'ground_truth')\n",
    "\n",
    "# This file contains the document labels computed by the Mechanical Turk\n",
    "# workers.\n",
    "# TODO(andrei) There can be contradictions, right?\n",
    "# Unlike the NIST expert judgement file, this one is provided from the \n",
    "# beginning as just one file (yay!). Contains the development data for the\n",
    "# second part of the challenge (consensus).\n",
    "WORKER_LABEL_FILE = os.path.join(DATA_ROOT, 'stage2-dev', 'stage2.dev')\n",
    "\n",
    "# Mechanical Turk worker judgements for the 2011 Crowdsourcing Track. \n",
    "JUDGEMENT_FILE = os.path.join(DATA_ROOT, 'all_judgements.tsv')\n",
    "\n",
    "# Provided test data for the 1st stage of the TREC 2011 Crowdsourcing Track.\n",
    "TEST_LABEL_FILE_SHARED = os.path.join(DATA_ROOT, 'test-set-Aug-8', 'trec-cs-2011-test-set-shared.csv')\n",
    "TEST_LABEL_FILE_TEAMS = os.path.join(DATA_ROOT, 'test-set-Aug-8', 'trec-cs-2011-test-set-assigned-to-teams.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class JudgementRecord(object):\n",
    "    \"\"\" Judgement record submitted in the 2011 Crowdsourcing Track.\n",
    "    \n",
    "        Attributes:\n",
    "            label_type: Additional label metadata (enum).\n",
    "                0: default\n",
    "                1. rejected label: where you would have filtered this \n",
    "                label out before subsequent use\n",
    "                2. automated label: label was produced by automation \n",
    "                (.artificial artificial artificial intelligence.)\n",
    "                3. training / quality-control label: used in training/evaluating\n",
    "                worker, not for labeling test data\n",
    "    \"\"\"\n",
    "    def __init__(self, table_row):\n",
    "        attributes = table_row.split('\\t')\n",
    "        team_id, worker_id, _, topic_id, doc_id, _, relevance, _, _, _, label_type = attributes\n",
    "        self.team_id = team_id\n",
    "        self.worker_id = worker_id\n",
    "        self.label_type = int(label_type)\n",
    "        self.topic_id = topic_id\n",
    "        self.doc_id = doc_id\n",
    "        if not relevance == 'na':\n",
    "            self.is_relevant = (float(relevance) >= 0.5)\n",
    "        else:\n",
    "            self.is_relevant = None\n",
    "            \n",
    "    def is_useful(self):\n",
    "        return self.label_type == 0 and (self.is_relevant is not None)\n",
    "        \n",
    "            \n",
    "class WorkerLabel(object):\n",
    "    def __init__(self, table_row):\n",
    "        attributes = table_row.split()\n",
    "        topic_id, hit_id, worker_id, document_id, nist_label, worker_label = attributes\n",
    "        self.topic_id = topic_id\n",
    "        self.hit_id = hit_id\n",
    "        self.worker_id = worker_id\n",
    "        self.document_id = document_id\n",
    "        self.nist_label = nist_label\n",
    "        self.worker_label = worker_label\n",
    "        \n",
    "        \n",
    "class ExpertLabel(object):\n",
    "    def __init__(self, attributes):\n",
    "        if len(attributes) == 3:\n",
    "            topic_id, document_id, label = attributes\n",
    "        elif len(attributes) == 4:\n",
    "            # Also includes set column, which we ignore\n",
    "            _, topic_id, document_id, label = attributes\n",
    "        elif len(attributes) == 5:\n",
    "            # Also includes team and set columns, which we ignore\n",
    "            _, _, topic_id,document_id, label = attributes\n",
    "        else:\n",
    "            raise Exception(\"Unsupported expert label format: [%s]\" % table_row)\n",
    "        \n",
    "        self.topic_id = topic_id\n",
    "        self.document_id = document_id\n",
    "        # 0 (non-relevant), 1 (relevant) or 2 (highly relevant)\n",
    "        self.label = int(label)\n",
    "        \n",
    "    def is_relevant(self):\n",
    "        return self.label > 0\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"%s:%s:%s\" % (self.topic_id, self.document_id, \"Relevant\" if self.is_relevant() else \"Not relevant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_judgement_labels(file_name):\n",
    "    with io.open(file_name, 'r') as f:\n",
    "        return [JudgementRecord(line[:-1]) for line in f]\n",
    "            \n",
    "def read_expert_labels(file_name, header=False, sep=None):\n",
    "    with io.open(file_name, 'r') as f:\n",
    "        if header:\n",
    "            # Skip the header\n",
    "            f.readline()\n",
    "        return [ExpertLabel(line.split(sep)) for line in f]\n",
    "\n",
    "def read_worker_labels(file_name):\n",
    "    with io.open(file_name, 'r') as f:\n",
    "        return [WorkerLabel(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2033 NIST expert labels\n"
     ]
    }
   ],
   "source": [
    "expert_labels = read_expert_labels(EXPERT_GROUND_TRUTH_FILE)\n",
    "print(\"%d NIST expert labels\" % len(expert_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10770 Mechanical Turk worker labels\n"
     ]
    }
   ],
   "source": [
    "worker_labels = read_worker_labels(WORKER_LABEL_FILE)\n",
    "print(\"%d Mechanical Turk worker labels\" % len(worker_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244 topics in NIST expert label data\n"
     ]
    }
   ],
   "source": [
    "expert_label_topic_ids = { l.topic_id for l in expert_labels }\n",
    "print(\"%d topics in NIST expert label data\" % len(expert_label_topic_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 topics in development worker label data\n"
     ]
    }
   ],
   "source": [
    "worker_label_topic_ids = { l.topic_id for l in worker_labels }\n",
    "print(\"%d topics in development worker label data\" % len(worker_label_topic_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is it normal to have 25 topics in worker labels, but 244 topics in expert labels? (stage1-dev and stage2-dev READMEs confirm these counts!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'24 topics in common (NIST expert labels and development worker labels)'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_expert_worker_topic_ids = expert_label_topic_ids & worker_label_topic_ids\n",
    "str(len(common_expert_worker_topic_ids)) + ' topics in common (NIST expert labels and development worker labels)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'64042 judgement labels'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "judgement_labels_2011 = read_judgement_labels(JUDGEMENT_FILE)\n",
    "str(len(judgement_labels_2011)) + ' judgement labels'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2011 Judgement Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "judgement_topic_ids = { l.topic_id for l in judgement_labels_2011 }\n",
    "len(judgement_topic_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(judgement_topic_ids & expert_label_topic_ids))\n",
    "print(len(judgement_topic_ids & worker_label_topic_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Clear out labels deemed irrelevant (e.g. ones used for worker assessment).\n",
    "\n",
    "useful_judgement_labels_2011 = [l for l in judgement_labels_2011 if l.is_useful()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 different topics in 2011 judgement data\n",
      "2 topics in common between 2011 judgement data and original NISP expert label data.\n",
      "0 topics in common between 2011 judgement data and original (dev) worker label data.\n"
     ]
    }
   ],
   "source": [
    "useful_judgement_topic_ids = { l.topic_id for l in useful_judgement_labels_2011 }\n",
    "print(\"%d different topics in 2011 judgement data\" % len(useful_judgement_topic_ids))\n",
    "print(\"%d topics in common between 2011 judgement data and original NISP expert label data.\" %\n",
    "      len(useful_judgement_topic_ids & expert_label_topic_ids))\n",
    "print(\"%d topics in common between 2011 judgement data and original (dev) worker label data.\" % \n",
    "      len(useful_judgement_topic_ids & worker_label_topic_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2011 Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1655\n",
      "First 5:\n",
      "20542:clueweb09-en0003-47-17392:Relevant\n",
      "20542:clueweb09-en0002-74-25816:Relevant\n",
      "20542:clueweb09-en0000-00-00000:Not relevant\n",
      "20542:clueweb09-enwp00-69-12844:Relevant\n",
      "20542:clueweb09-en0002-93-19628:Relevant\n",
      "Last 5:\n",
      "20996:clueweb09-en0129-94-14964:Not relevant\n",
      "20996:clueweb09-en0129-94-14966:Not relevant\n",
      "20996:clueweb09-en0131-42-22886:Not relevant\n",
      "20996:clueweb09-en0132-77-26392:Not relevant\n",
      "20996:clueweb09-enwp01-17-03021:Not relevant\n",
      "Last 5 (after merge):\n",
      "20958:clueweb09-en0112-59-01254:Not relevant\n",
      "20958:clueweb09-en0114-14-25526:Not relevant\n",
      "20958:clueweb09-en0116-09-14871:Not relevant\n",
      "20958:clueweb09-en0116-09-14873:Not relevant\n",
      "20958:clueweb09-en0121-92-02032:Not relevant\n"
     ]
    }
   ],
   "source": [
    "test_data_shared = read_expert_labels(TEST_LABEL_FILE_SHARED, header=True, sep=',')\n",
    "test_data_team = read_expert_labels(TEST_LABEL_FILE_TEAMS, header=True, sep=',')\n",
    "\n",
    "print(len(test_data_shared))\n",
    "print(\"First 5:\\n\" + \"\\n\".join([str(d) for d in test_data_shared[:5]]))\n",
    "print(\"Last 5:\\n\" + \"\\n\".join([str(d) for d in test_data_shared[-5:]]))\n",
    "\n",
    "test_data = test_data_shared + test_data_team\n",
    "print(\"Last 5 (after merge):\")\n",
    "print(\"\\n\".join([str(d) for d in test_data[-5:]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 different topics in test data.\n"
     ]
    }
   ],
   "source": [
    "test_topic_ids = { l.topic_id for l in test_data }\n",
    "print(\"%d different topics in test data.\" % len(test_topic_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "print(len(test_topic_ids & useful_judgement_topic_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(len(test_topic_ids & expert_label_topic_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(test_topic_ids & worker_label_topic_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    " * Full topic overlap between judgement data and test data.\n",
    " * 6% (2/30) topic overlap between expert label data and test data.\n",
    " * 0% (0/30) topic overlap between original worker consensus training data labels and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
