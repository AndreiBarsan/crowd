{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Data Overview\n",
    "\n",
    "This notebook contains some general info regarding the data available for this project.\n",
    "\n",
    "The judgements used by this project are the NIST expert judgements ('stage1-dev') and the consensus labels ('stage2-dev') from [the 2011 TREC Crowdsourcing track](https://sites.google.com/site/treccrowd/2011).\n",
    "\n",
    "The actual document data used is, sadly, not publicly available (http://lemurproject.org/clueweb09/, ClueWeb09 dataset, T11Crowd subsection), but can be acquired by signing a non-commercial use agreement with the provider."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the data management code has been cannibalized from [Martin Davtyan's previous work on the subject](https://github.com/martinthenext/ir-crowd-thesis) (while at ETH Zurich)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This should be the root folder containing the different judgement datasets.\n",
    "DATA_ROOT = os.path.join(os.getenv('HOME'), 'data')\n",
    "\n",
    "# This file contains exclusively the NIST expert judgements from 'stage1-dev'\n",
    "# 'cat'ed together into a single file (from all the teams, as well as the\n",
    "# common data).\n",
    "EXPERT_GROUND_TRUTH_FILE = os.path.join(DATA_ROOT, 'ground_truth')\n",
    "\n",
    "# This file contains the document labels computed by the Mechanical Turk\n",
    "# workers.\n",
    "# TODO(andrei) There can be contradictions, right?\n",
    "# Unlike the NIST expert judgement file, this one is provided from the \n",
    "# beginning as just one file (yay!). Contains the development data for the\n",
    "# second part of the challenge (consensus).\n",
    "WORKER_LABEL_FILE = os.path.join(DATA_ROOT, 'stage2-dev', 'stage2.dev')\n",
    "\n",
    "# Mechanical Turk worker judgements for the 2011 Crowdsourcing Track. \n",
    "JUDGEMENT_FILE = os.path.join(DATA_ROOT, 'all_judgements.tsv')\n",
    "\n",
    "# Provided test data for the 1st stage of the TREC 2011 Crowdsourcing Track.\n",
    "TEST_LABEL_FILE_SHARED = os.path.join(DATA_ROOT, 'test-set-Aug-8', 'trec-cs-2011-test-set-shared.csv')\n",
    "TEST_LABEL_FILE_TEAMS = os.path.join(DATA_ROOT, 'test-set-Aug-8', 'trec-cs-2011-test-set-assigned-to-teams.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This loads the necessary data wrangling classes and functions.\n",
    "# I\n",
    "%run ../data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_judgement_labels(file_name):\n",
    "    with io.open(file_name, 'r') as f:\n",
    "        return [JudgementRecord(line[:-1]) for line in f]\n",
    "            \n",
    "def read_expert_labels(file_name, header=False, sep=None):\n",
    "    with io.open(file_name, 'r') as f:\n",
    "        if header:\n",
    "            # Skip the header\n",
    "            f.readline()\n",
    "        return [ExpertLabel(line.split(sep)) for line in f]\n",
    "\n",
    "def read_worker_labels(file_name):\n",
    "    with io.open(file_name, 'r') as f:\n",
    "        return [WorkerLabel(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "expert_labels = read_expert_labels(EXPERT_GROUND_TRUTH_FILE)\n",
    "print(\"%d NIST expert labels\" % len(expert_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "worker_labels = read_worker_labels(WORKER_LABEL_FILE)\n",
    "print(\"%d Mechanical Turk worker labels\" % len(worker_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "expert_label_topic_ids = { l.topic_id for l in expert_labels }\n",
    "print(\"%d topics in NIST expert label data\" % len(expert_label_topic_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "worker_label_topic_ids = { l.topic_id for l in worker_labels }\n",
    "print(\"%d topics in development worker label data\" % len(worker_label_topic_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is it normal to have 25 topics in worker labels, but 244 topics in expert labels? (stage1-dev and stage2-dev READMEs confirm these counts!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "common_expert_worker_topic_ids = expert_label_topic_ids & worker_label_topic_ids\n",
    "str(len(common_expert_worker_topic_ids)) + ' topics in common (NIST expert labels and development worker labels)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "judgement_labels_2011 = read_judgement_labels(JUDGEMENT_FILE)\n",
    "str(len(judgement_labels_2011)) + ' judgement labels'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2011 Judgement Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "judgement_topic_ids = { l.topic_id for l in judgement_labels_2011 }\n",
    "len(judgement_topic_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(judgement_topic_ids & expert_label_topic_ids))\n",
    "print(len(judgement_topic_ids & worker_label_topic_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Clear out labels deemed irrelevant (e.g. ones used for worker assessment).\n",
    "\n",
    "useful_judgement_labels_2011 = [l for l in judgement_labels_2011 if l.is_useful()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "useful_judgement_topic_ids = { l.topic_id for l in useful_judgement_labels_2011 }\n",
    "print(\"%d different topics in 2011 judgement data\" % len(useful_judgement_topic_ids))\n",
    "print(\"%d topics in common between 2011 judgement data and original NIST expert label data.\" %\n",
    "      len(useful_judgement_topic_ids & expert_label_topic_ids))\n",
    "print(\"%d topics in common between 2011 judgement data and original (dev) worker label data.\" % \n",
    "      len(useful_judgement_topic_ids & worker_label_topic_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2011 Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data_shared = read_expert_labels(TEST_LABEL_FILE_SHARED, header=True, sep=',')\n",
    "test_data_team = read_expert_labels(TEST_LABEL_FILE_TEAMS, header=True, sep=',')\n",
    "\n",
    "print(len(test_data_shared))\n",
    "print(\"First 5:\\n\" + \"\\n\".join([str(d) for d in test_data_shared[:5]]))\n",
    "print(\"Last 5:\\n\" + \"\\n\".join([str(d) for d in test_data_shared[-5:]]))\n",
    "\n",
    "test_data = test_data_shared + test_data_team\n",
    "print(\"Last 5 (after merge):\")\n",
    "print(\"\\n\".join([str(d) for d in test_data[-5:]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_topic_ids = { l.topic_id for l in test_data }\n",
    "print(\"%d different topics in test data.\" % len(test_topic_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(test_topic_ids & useful_judgement_topic_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(test_topic_ids & expert_label_topic_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(test_topic_ids & worker_label_topic_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    " * Full topic overlap between judgement data and test data.\n",
    " * 6% (2/30) topic overlap between expert label data and test data.\n",
    " * 0% (0/30) topic overlap between original worker consensus training data labels and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
