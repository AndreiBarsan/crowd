{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Data Overview\n",
    "\n",
    "This notebook contains some general info regarding the data available for this project.\n",
    "\n",
    "The judgements used by this project are the NIST expert judgements ('stage1-dev') and the consensus labels ('stage2-dev') from [the 2011 TREC Crowdsourcing track](https://sites.google.com/site/treccrowd/2011).\n",
    "\n",
    "The actual document data used is, sadly, not publicly available (http://lemurproject.org/clueweb09/, ClueWeb09 dataset, T11Crowd subsection), but can be acquired by signing a non-commercial use agreement with the provider."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the data management code has been cannibalized from [Martin Davtyan's previous work on the subject](https://github.com/martinthenext/ir-crowd-thesis) (while at ETH Zurich)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some of the numbers in this notebook may be out of date, since the old files have been moved around to have new, clearer names, and most of the confusion has been resolved.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This makes Jupyter pretend to be Pythonic and play well with modules.\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.expandvars(os.path.join(os.getcwd(), '..')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "from crowd.data import *\n",
    "from crowd.config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if 'notebooks' in os.getcwd():\n",
    "    print(os.getcwd())\n",
    "    os.chdir('..')\n",
    "    print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ground_truth = read_ground_truth()\n",
    "turk_labels = read_useful_judgement_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ground_truth_topics = {t.topic_id for t in ground_truth}\n",
    "turk_label_topics = {lbl.topic_id for lbl in turk_labels}\n",
    "\n",
    "assert len(ground_truth_topics & turk_label_topics) == 30, \\\n",
    "    \"All 30 topics must be covered by the ground truth and label data!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "judgements_by_doc_id = get_all_judgements_by_doc_id(turk_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3195\n"
     ]
    }
   ],
   "source": [
    "print(len(judgements_by_doc_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels_by_topic_doc = {}\n",
    "for label in turk_labels:\n",
    "    if label.topic_id not in labels_by_topic_doc:\n",
    "        labels_by_topic_doc[label.topic_id] = {}\n",
    "        \n",
    "    if label.doc_id not in labels_by_topic_doc[label.topic_id]:\n",
    "        labels_by_topic_doc[label.topic_id][label.doc_id] = []\n",
    "        \n",
    "    labels_by_topic_doc[label.topic_id][label.doc_id].append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "dict_keys(['20958', '20956', '20996', '20686', '20696', '20814', '20488', '20764', '20694', '20976', '20714', '20704', '20916', '20424', '20778', '20922', '20542', '20972', '20690', '20910', '20584', '20766', '20780', '20962', '20644', '20832', '20636', '20812', '20932', '20642'])\n"
     ]
    }
   ],
   "source": [
    "print(len(labels_by_topic_doc))\n",
    "print(labels_by_topic_doc.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#20958\t Avg. labels per labeled doc: 8.60; Total votes: 860\n",
      "#20956\t Avg. labels per labeled doc: 20.62; Total votes: 2268\n",
      "#20996\t Avg. labels per labeled doc: 21.15; Total votes: 2327\n",
      "#20686\t Avg. labels per labeled doc: 20.79; Total votes: 2391\n",
      "#20696\t Avg. labels per labeled doc: 4.05; Total votes: 446\n",
      "#20814\t Avg. labels per labeled doc: 9.38; Total votes: 938\n",
      "#20488\t Avg. labels per labeled doc: 12.36; Total votes: 1360\n",
      "#20764\t Avg. labels per labeled doc: 11.55; Total votes: 1155\n",
      "#20694\t Avg. labels per labeled doc: 19.48; Total votes: 1948\n",
      "#20976\t Avg. labels per labeled doc: 11.93; Total votes: 1074\n",
      "#20714\t Avg. labels per labeled doc: 6.21; Total votes: 683\n",
      "#20704\t Avg. labels per labeled doc: 5.17; Total votes: 465\n",
      "#20916\t Avg. labels per labeled doc: 7.86; Total votes: 865\n",
      "#20424\t Avg. labels per labeled doc: 3.90; Total votes: 390\n",
      "#20778\t Avg. labels per labeled doc: 21.72; Total votes: 2389\n",
      "#20922\t Avg. labels per labeled doc: 1.05; Total votes: 105\n",
      "#20542\t Avg. labels per labeled doc: 21.80; Total votes: 2507\n",
      "#20972\t Avg. labels per labeled doc: 21.03; Total votes: 1893\n",
      "#20690\t Avg. labels per labeled doc: 20.58; Total votes: 2573\n",
      "#20910\t Avg. labels per labeled doc: 21.37; Total votes: 2458\n",
      "#20584\t Avg. labels per labeled doc: 19.49; Total votes: 2046\n",
      "#20766\t Avg. labels per labeled doc: 6.91; Total votes: 760\n",
      "#20780\t Avg. labels per labeled doc: 21.26; Total votes: 2445\n",
      "#20962\t Avg. labels per labeled doc: 14.73; Total votes: 1620\n",
      "#20644\t Avg. labels per labeled doc: 1.86; Total votes: 195\n",
      "#20832\t Avg. labels per labeled doc: 7.22; Total votes: 722\n",
      "#20636\t Avg. labels per labeled doc: 22.09; Total votes: 2209\n",
      "#20812\t Avg. labels per labeled doc: 21.30; Total votes: 2449\n",
      "#20932\t Avg. labels per labeled doc: 20.53; Total votes: 2361\n",
      "#20642\t Avg. labels per labeled doc: 21.02; Total votes: 2417\n"
     ]
    }
   ],
   "source": [
    "for tid, doc_map in labels_by_topic_doc.items():\n",
    "    s = 0\n",
    "    for doc_id, labels in doc_map.items():\n",
    "        if len(labels) == 0:\n",
    "            continue\n",
    "            \n",
    "        s += len(labels)\n",
    "        \n",
    "    avg_labels_labeled = s / len(doc_map)\n",
    "    print(\"#{0}\\t Avg. labels per labeled doc: {1:.2f}; Total votes: {2}\"\n",
    "          .format(tid, avg_labels_labeled, s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is the old data (not used in project)\n",
    "\n",
    "This code evaluates the overlap and usefulness of several different datasets I downloaded separately, in an attempt to establish which are the correct ones. The issue is now solved and the methods used in the above section should be used when working with the 2011 corpus used in my, Martin's, or Piyush's paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9380 NIST expert labels\n"
     ]
    }
   ],
   "source": [
    "expert_labels = read_expert_labels(EXPERT_GROUND_TRUTH_FILE, header=True, sep=',')\n",
    "print(\"%d NIST expert labels\" % len(expert_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10770 Mechanical Turk worker labels\n"
     ]
    }
   ],
   "source": [
    "worker_labels = read_worker_labels(WORKER_LABEL_FILE)\n",
    "print(\"%d Mechanical Turk worker labels\" % len(worker_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 topics in NIST expert label data\n"
     ]
    }
   ],
   "source": [
    "expert_label_topic_ids = { l.topic_id for l in expert_labels }\n",
    "print(\"%d topics in NIST expert label data\" % len(expert_label_topic_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 topics in development worker label data\n"
     ]
    }
   ],
   "source": [
    "worker_label_topic_ids = { l.topic_id for l in worker_labels }\n",
    "print(\"%d topics in development worker label data\" % len(worker_label_topic_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0 topics in common (ground truth expert labels and development worker labels)'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_expert_worker_topic_ids = expert_label_topic_ids & worker_label_topic_ids\n",
    "str(len(common_expert_worker_topic_ids)) + ' topics in common (ground truth expert labels and development worker labels)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'64042 judgement labels'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "judgement_labels_2011 = read_judgement_labels(JUDGEMENT_FILE)\n",
    "str(len(judgement_labels_2011)) + ' judgement labels'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The topic overlap is to be expected, since the expert label data is from 2011, and the worker label data is older, from an entirely different session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2011 Judgement Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "judgement_topic_ids = { l.topic_id for l in judgement_labels_2011 }\n",
    "len(judgement_topic_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(judgement_topic_ids & expert_label_topic_ids))\n",
    "print(len(judgement_topic_ids & worker_label_topic_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Clear out labels deemed irrelevant (e.g. ones used for worker assessment).\n",
    "useful_judgement_labels_2011 = read_useful_judgement_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 different topics in 2011 judgement data\n",
      "30 topics in common between 2011 judgement data and original NIST expert label data.\n",
      "0 topics in common between 2011 judgement data and original (dev) worker label data.\n"
     ]
    }
   ],
   "source": [
    "useful_judgement_topic_ids = { l.topic_id for l in useful_judgement_labels_2011 }\n",
    "print(\"%d different topics in 2011 judgement data\" % len(useful_judgement_topic_ids))\n",
    "print(\"%d topics in common between 2011 judgement data and original NIST expert label data.\" %\n",
    "      len(useful_judgement_topic_ids & expert_label_topic_ids))\n",
    "print(\"%d topics in common between 2011 judgement data and original (dev) worker label data.\" % \n",
    "      len(useful_judgement_topic_ids & worker_label_topic_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2011 Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1655\n",
      "First 5:\n",
      "20542:clueweb09-en0003-47-17392:Relevant\n",
      "20542:clueweb09-en0002-74-25816:Relevant\n",
      "20542:clueweb09-en0000-00-00000:Non-relevant\n",
      "20542:clueweb09-enwp00-69-12844:Relevant\n",
      "20542:clueweb09-en0002-93-19628:Relevant\n",
      "Last 5:\n",
      "20996:clueweb09-en0129-94-14964:Unknown\n",
      "20996:clueweb09-en0129-94-14966:Unknown\n",
      "20996:clueweb09-en0131-42-22886:Unknown\n",
      "20996:clueweb09-en0132-77-26392:Unknown\n",
      "20996:clueweb09-enwp01-17-03021:Unknown\n",
      "Last 5 (after merge):\n",
      "20958:clueweb09-en0112-59-01254:Unknown\n",
      "20958:clueweb09-en0114-14-25526:Unknown\n",
      "20958:clueweb09-en0116-09-14871:Unknown\n",
      "20958:clueweb09-en0116-09-14873:Unknown\n",
      "20958:clueweb09-en0121-92-02032:Unknown\n"
     ]
    }
   ],
   "source": [
    "test_data_shared = read_expert_labels(TEST_LABEL_FILE_SHARED, header=True, sep=',')\n",
    "test_data_team = read_expert_labels(TEST_LABEL_FILE_TEAMS, header=True, sep=',')\n",
    "\n",
    "print(len(test_data_shared))\n",
    "print(\"First 5:\\n\" + \"\\n\".join([str(d) for d in test_data_shared[:5]]))\n",
    "print(\"Last 5:\\n\" + \"\\n\".join([str(d) for d in test_data_shared[-5:]]))\n",
    "\n",
    "test_data = test_data_shared + test_data_team\n",
    "print(\"Last 5 (after merge):\")\n",
    "print(\"\\n\".join([str(d) for d in test_data[-5:]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 different topics in test data.\n"
     ]
    }
   ],
   "source": [
    "test_topic_ids = { l.topic_id for l in test_data }\n",
    "print(\"%d different topics in test data.\" % len(test_topic_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "print(len(test_topic_ids & useful_judgement_topic_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "print(len(test_topic_ids & expert_label_topic_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(test_topic_ids & worker_label_topic_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ground truth stats (old)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Also including non-ground-truth labels (-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9380] total entries in test data.\n",
      "[3195] Unique document IDs in test data.\n",
      "[30] Unique topic IDs in test data.\n",
      "[3200] Unique judgements in test data.\n"
     ]
    }
   ],
   "source": [
    "print(\"[%d] total entries in test data.\" % len(test_data))\n",
    "test_data_unique_docs = {l.document_id for l in test_data}\n",
    "test_data_unique_topics = {l.topic_id for l in test_data}\n",
    "print(\"[%d] Unique document IDs in test data.\" % len(test_data_unique_docs))\n",
    "print(\"[%d] Unique topic IDs in test data.\" % len(test_data_unique_topics))\n",
    "\n",
    "test_data_unique_points = {(l.topic_id, l.document_id) for l in test_data}\n",
    "print(\"[%d] Unique judgements in test data.\" % len(test_data_unique_points))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering out non-ground-truth labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1015] Useful data points in test data (with labels != -1).\n",
      "[30] Topic IDs in useful data.\n",
      "[394] Document IDs in useful data.\n",
      "[395] Unique useful data points.\n"
     ]
    }
   ],
   "source": [
    "useful_test_dp = [l for l in test_data if l.label != -1]\n",
    "print(\"[%d] Useful data points in test data (with labels != -1).\" % len(useful_test_dp))\n",
    "print(\"[%d] Topic IDs in useful data.\" % len({l.topic_id for l in useful_test_dp}))\n",
    "print(\"[%d] Document IDs in useful data.\" % len({l.document_id for l in useful_test_dp}))\n",
    "unique_useful_test_dp = {(l.topic_id, l.document_id) for l in useful_test_dp}\n",
    "print(\"[%d] Unique useful data points.\" % len(unique_useful_test_dp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
