{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Diffusion for Vote Selection\n",
    "\n",
    "This notebook explores techniques from heat diffusion (e.g. used to model information diffusion in social networks in compute science) to compute areas of maximum uncertainty in the document graph.\n",
    "\n",
    "This algorithm is used to select which unvoted (document) nodes to request votes for. It is quite effective at selecting the most useful documents thereby increasing the accuracy of the system with even a very low budget."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Greedy algorithm for submodular function maximization, i.e. coverage, over the independent cascades (IC) model:\n",
    "\n",
    " * For budget = 1 to n_documents\n",
    "   * Simulate process k times for every new candidate, and pick candidate which, on average (i.e. in expectation), when added to the seed set maximizes the spread caused by the seed set\n",
    "\n",
    "Request these votes, and aggregate in a fixed way (e.g. MV or MVNN).\n",
    "\n",
    "TODO(andrei): Integrate this code into the Gaussian Processes framework for sensible comparison with other research in the field. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Python: /Users/andrei/anaconda3/envs/crowd/bin/python\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# If you're using Anaconda and this doesn't correspond to the correct environment\n",
    "# (i.e. the one active when 'jupyter notebook' was called), run\n",
    "#     $ ipython3 kernel install \n",
    "# and restart the notebook.\n",
    "print(\"Current Python: {0}\".format(sys.executable))\n",
    "\n",
    "# This makes Jupyter pretend to be Pythonic and play well with modules.\n",
    "sys.path.append(os.path.expandvars(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from collections import OrderedDict\n",
    "from datetime import datetime\n",
    "import io\n",
    "import os\n",
    "import random\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "from crowd.aggregation import *\n",
    "from crowd.config import *\n",
    "from crowd.data import *\n",
    "from crowd.file_util import *\n",
    "from crowd.graph import *\n",
    "from crowd.graph_sampling import *\n",
    "from crowd.simulation import *\n",
    "from crowd.topic import *\n",
    "from crowd.util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make sure we operate in the same directory as the other tools\n",
    "# in the project.\n",
    "if 'notebooks' in os.getcwd():\n",
    "    print(os.getcwd())\n",
    "    os.chdir('..')\n",
    "    print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building nx doc graph\n",
      "7 relevant documents\n",
      "3 non-relevant documents\n",
      "100 documents with votes\n",
      "66 hidden nodes (due to no data)\n",
      "100\n",
      "100\n",
      "Will DISCARD empty nodes in the graph.\n",
      "Nodes: 100\n",
      "Edges: 62\n"
     ]
    }
   ],
   "source": [
    "SIM_THRESHOLD = 0.75\n",
    "DISCARD_EMPTY_NODES = True\n",
    "\n",
    "id_topic_info = load_topic_metadata()\n",
    "judgements = read_useful_judgement_labels(JUDGEMENT_FILE)\n",
    "test_data = read_all_test_labels()\n",
    "\n",
    "# Pick one single topic to mess around with at the moment.\n",
    "# Topic: 20814 Elvish Language\n",
    "topic_id = '20814'\n",
    "# topic_id = '20704'\n",
    "graph = build_nx_document_graph(\n",
    "    id_topic_info[topic_id],\n",
    "    test_data,\n",
    "    get_topic_judgements_by_doc_id(topic_id, judgements),\n",
    "    FULLTEXT_FOLDER,\n",
    "    sim_threshold=SIM_THRESHOLD,\n",
    "    discard_empty=DISCARD_EMPTY_NODES)\n",
    "classic_graph = build_document_graph(\n",
    "    id_topic_info[topic_id],\n",
    "    FULLTEXT_FOLDER,\n",
    "    sim_threshold=SIM_THRESHOLD)\n",
    "topic_judgements = get_topic_judgements_by_doc_id(topic_id, judgements)\n",
    "topic_ground_truth = {truth.document_id: truth for truth in test_data\n",
    "                          if truth.topic_id == topic_id}\n",
    "print(len(topic_judgements.keys()))\n",
    "print(len(topic_ground_truth.keys()))\n",
    "\n",
    "# Important note: Not discarding empty nodes significantly slows the algorithm\n",
    "# down, since the information propagation must then be computed on a much larger\n",
    "# graph.\n",
    "if DISCARD_EMPTY_NODES:\n",
    "    print(\"Will DISCARD empty nodes in the graph.\")\n",
    "else:        \n",
    "    # TODO(andrei): Ensure that when we're sampling, we DON'T try to sample\n",
    "    # any of the votes which have not recorded votes.\n",
    "    print(\"Not discarding empty nodes\")\n",
    "    \n",
    "print(\"Nodes: {}\".format(graph.nx_graph.number_of_nodes()))\n",
    "print(\"Edges: {}\".format(graph.nx_graph.number_of_edges()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected components: 68\n",
      "Cliques:              70\n"
     ]
    }
   ],
   "source": [
    "# This is just a sanity check preview. It's not nearly\n",
    "# as useful as the one in the dedicated similarity graph\n",
    "# visualization notebook.\n",
    "# TODO(andrei): Consider always working with NetworkX graphs directly, if it makes sense.\n",
    "\n",
    "# from networkx.drawing.nx_agraph import graphviz_layout\n",
    "# pos = graphviz_layout(graph.nx_graph)\n",
    "# nx.draw(graph.nx_graph, pos, node_size=2500, node_color='w')\n",
    "# _ = nx.draw_networkx_labels(graph.nx_graph, pos)\n",
    "# # _ = nx.draw_networkx_edge_labels(graph.nx_graph, pos)\n",
    "# nx.draw_networkx_edges(graph.nx_graph, pos)\n",
    "\n",
    "print(\"Connected components: {}\".format(nx.number_connected_components(graph.nx_graph)))\n",
    "print(\"Cliques:              {}\".format(nx.graph_number_of_cliques(graph.nx_graph)))\n",
    "\n",
    "# nx.draw(graph.nx_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ng = graph.nx_graph\n",
    "n_docs = ng.number_of_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{clueweb09-en0020-09-05220: {'similarity': 0.83260761009306117},\n",
       " clueweb09-en0022-00-14843: {'similarity': 0.82834612080853465},\n",
       " clueweb09-en0027-48-27832: {'similarity': 0.95115279699710087},\n",
       " clueweb09-en0035-25-25842: {'similarity': 0.83333782655166722},\n",
       " clueweb09-en0065-64-28203: {'similarity': 0.78686207716967493},\n",
       " clueweb09-en0112-39-04197: {'similarity': 0.9701917457905459}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting a particular node's neighbors.\n",
    "# Note that the actual node objects are of type 'NxDocumentNode'.\n",
    "ng['clueweb09-en0007-20-27316']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO list\n",
    " * Problem: What to do once we reach 100% coverage, i.e. 1 vote for every document?\n",
    "     * We encounter a similar problem to GP aggregation, cyclicity. Namely, influence spread is maximized when we have sampled every node exactly once, but this doesn't mean that we should be 100% confident our predictions are correct.\n",
    "     * We need a smart way to model uncertainty beyond 1 vote per doc.\n",
    "     * Possible solution: Remodel influence maximization problem so that every node spreads information based on its own confidence. If there is consent among voters so far, then its weight is higher than if there is dissent.\n",
    " * Write script for one-command deployment to Euler (hint: use pyinvoke).\n",
    " * Model IC/LT configs in a neat OO fashion.\n",
    " * Consider estimating the number of tries necessary if using a brute-force approach. For instance, for $n = 150$ documents and a budget $k = b = 15$ we would have to check about $10^{20}$ possible subsets. And this doesn't even count possible uncertainties and needing >1 vote per document to be confident in its relevance. We need submodularity! The **k-max cover problem** is NP-hard.\n",
    " \n",
    " \n",
    "### Done\n",
    " * Simulate one iteration of influence maximization under IC.\n",
    " * Create function which repeats simulation k several times, and computes the expected influence of a given seed set.\n",
    " * Create function which, given a seed set, finds the best addition to that seed set using the above function.\n",
    " * Create function which, given a graph and a budget b, computes the approximately best solution to maximize influence.\n",
    "     * These will be the votes we will request.\n",
    " * Plot learning curves for this system, e.g. from budget = 1, up to budget = n_docs, or even n_docs * nr_votes_per doc.\n",
    " * This technique is VERY CPU-intensive, but also embarrassingly parallel, so we should parallelize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lazy Greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Budget: 1/15, Spread: -8.00\n",
      "Budget: 2/15, Spread: -14.00\n",
      "Budget: 3/15, Spread: -18.00\n",
      "Budget: 4/15, Spread: -22.00\n",
      "Budget: 5/15, Spread: -25.00\n",
      "Budget: 6/15, Spread: -27.00\n",
      "Budget: 7/15, Spread: -29.00\n",
      "Budget: 8/15, Spread: -29.00\n",
      "Budget: 9/15, Spread: -32.70\n",
      "Budget: 10/15, Spread: -34.80\n",
      "Budget: 11/15, Spread: -36.90\n",
      "Budget: 12/15, Spread: -36.90\n",
      "Budget: 13/15, Spread: -40.40\n",
      "Budget: 14/15, Spread: -42.60\n",
      "Budget: 15/15, Spread: -44.30\n",
      "Stats:  {'hit': 7, 'miss': 7}\n",
      " ['03-20962', '14-38989', '15-00571', '18-32052', '20-27316', '23-17065', '34-30758', '48-02434', '57-36727', '61-21414', '64-25797', '68-09061', '86-14243', '89-33025', '91-14026']\n"
     ]
    }
   ],
   "source": [
    "# General benchmarks; topic = 20814\n",
    "#\n",
    "#  * Very first timing,               b = 10, it = 10:    77639828 function calls in 26.211 seconds\n",
    "#  * Pre-cached node hash,            b = 10, it = 10:    49998191 function calls in 20.735, 21.717, 21.289 seconds\n",
    "#  * First buggy lazy greedy attempt, b = 10, it = 10:    25624964 function calls in 10.595 seconds\n",
    "#  * Second less buggy lg attempt,    b = 30, it = 10:    139919933 function calls in 57.896 seconds \n",
    "#    spread = 61.60\n",
    "# Fishy...\n",
    "#  * Non-lg                           b = 30, it = 10:    138283442 function calls in 57.391 seconds\n",
    "#    spread = 61.20\n",
    "\n",
    "\n",
    "# Clique-based investigation:\n",
    "#  lt  lazy greedy comparison:  {'miss': 12, 'hit': 5}, spread == 49.5\n",
    "#  lte lazy greedy comparison:  {'miss': 12, 'hit': 5} spread == 49.5\n",
    "#  epsilon 0.1 lazy greedy comparison: {'miss': 13, 'hit': 4}  Spread == 49.50\n",
    "#  epsilon 0.5 lazy greedy comparison: {'miss': 8, 'hit': 9}   Spread == 49.20\n",
    "#  epsilon 1.0 lazy greedy comparison: {'miss': 8, 'hit': 9}   Spread == 49.20\n",
    "#  epsilon 2.5 lazy greedy comparison: {'miss': 5, 'hit': 12}  Spread == 49.20\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (18, 10)\n",
    "\n",
    "random.seed(0xFFF000)\n",
    "%prun result = build_seed_set_lg(graph.nx_graph, 15, 10)\n",
    "clean_res = []\n",
    "for r in result:\n",
    "    d_id = r.document_id\n",
    "    d_id = d_id[d_id.find('-') + 1:]\n",
    "    d_id = d_id[d_id.find('-') + 1:]\n",
    "    clean_res.append(d_id)\n",
    "    \n",
    "print(sorted(clean_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    # Note: this function is likely incorrect.\n",
    "    result = build_seed_set(graph.nx_graph, 10, 10)\n",
    "    clean_res = []\n",
    "    for r in result:\n",
    "        d_id = r.document_id\n",
    "        d_id = d_id[d_id.find('-') + 1:]\n",
    "        d_id = d_id[d_id.find('-') + 1:]\n",
    "        clean_res.append(d_id)\n",
    "\n",
    "    print(sorted(clean_res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling + vote aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Least votes sampling\n",
    "print(\"Oldschool least-votes\")\n",
    "accuracies_mv_lowvotes = evaluate_iteration(\n",
    "    classic_graph,\n",
    "    topic_judgements,\n",
    "    topic_ground_truth,\n",
    "    LeastVotesSampler(),\n",
    "    aggregate_MV,\n",
    "    budget=len(topic_ground_truth.keys()))\n",
    "accuracies_mev_lowvotes = evaluate_iteration(\n",
    "    classic_graph,\n",
    "    topic_judgements,\n",
    "    topic_ground_truth,\n",
    "    LeastVotesSampler(),\n",
    "    aggregate_mev,\n",
    "    budget=len(topic_ground_truth.keys()))\n",
    "    \n",
    "# IC sampling\n",
    "print(\"IC\")\n",
    "accuracies_mev = evaluate_iteration(\n",
    "    graph,\n",
    "    topic_judgements,\n",
    "    topic_ground_truth,\n",
    "#     GraphSpreadSampler(graph, iteration_count=5),\n",
    "    LazyGreedyGraphSpreadSampler(graph, iteration_count=5),\n",
    "    aggregate_mev_nx,\n",
    "    budget=len(topic_ground_truth.keys()))\n",
    "accuracies_mv = evaluate_iteration(\n",
    "    graph,\n",
    "    topic_judgements,\n",
    "    topic_ground_truth,\n",
    "#     GraphSpreadSampler(graph, iteration_count=5),\n",
    "    LazyGreedyGraphSpreadSampler(graph, iteration_count=5),\n",
    "    aggregate_MV,\n",
    "    budget=len(topic_ground_truth.keys()))\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(range(len(accuracies_mev_lowvotes)), accuracies_mev_lowvotes, label=\"MEV classic\", marker='x')\n",
    "#plt.plot(range(len(accuracies_mv_lowvotes)), accuracies_mv_lowvotes, label=\"MV\", marker='x')\n",
    "\n",
    "plt.plot(range(len(accuracies_mev)), accuracies_mev, label=\"MEV information\", marker='^')\n",
    "#plt.plot(range(len(accuracies_mv)), accuracies_mv, label=\"MV\", marker='^')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing least-votes and graph-based sampling for topic: detroit riot (ID#20704)\n",
      "Building nx doc graph\n",
      "5 relevant documents\n",
      "5 non-relevant documents\n",
      "90 documents with votes\n",
      "51 hidden nodes (due to no data)\n",
      "Judgements: 90\n",
      "Ground truths: 90\n",
      "Computing results for IC Information Spread Sampler...\n"
     ]
    }
   ],
   "source": [
    "# Experimentation with the code from 'graph_sampling.py'\n",
    "compare_sampling('20704',\n",
    "                 sim_threshold=SIM_THRESHOLD,\n",
    "                 discard_empty_nodes=DISCARD_EMPTY_NODES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "compare_sampling('20814')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "compare_sampling('20932')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "compare_sampling('20832')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "compare_sampling('20956')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thoughts on also achieving voting confidence\n",
    "\n",
    " * Need **measure** of confidence in node.\n",
    " * Need to adapt sampling. With IC model, as soon as we reach a node with one edge, we \"activate\" it.\n",
    " * Linear Threshold (LT) model could be better? Then, a node would only \"activate\" if k neighbors (depending on similarity and its internal threshold) give it a heads-up. Still doesn't model confidence in votes in a particular node.\n",
    " * What if we re-weight the edges based the number of nodes? Still can't make the model want to pick a node more than once.\n",
    " * What about always picking e.g. 4 votes? If there's consent, we don't look at the node ever again until it's picked \"naturally\" again. If not, we want more votes there. Maybe just pick e.g. 4 votes again. If still no consent give up?\n",
    " \n",
    "Want to do global sampling based on confidence and informativeness. I.e. we want to weight informativeness by lack of confidence, in order to prioritize nodes where we have little confidence in their relevance.\n",
    "\n",
    "Compute top most informative (next) node as in the original plan by the new seed set spread, but then modulate every node's score by the confidence in that prediction. WON'T WORK because all candidates have never been sampled before by\n",
    "definition.\n",
    "\n",
    "How to extend information diffusion to allow multiple sampling of same node to further increase information flow?\n",
    "\n",
    "What does it mean to not have confidence in a document's relevance?\n",
    " * It has very few votes or none at all.\n",
    " * There's dissent among voters.\n",
    " * Want to design a `function(node, pos_votes, neg_votes)` which represents a measure of our confidence in its relevance.\n",
    " * We then want to use this somehow in conjunction with another sampling technique to pick the best node at a given time.\n",
    "     * How?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First algorithm testing. SIM_THRESHOLD = 0.75, topic = '20814'\n",
    "\n",
    "* B = 10, it = 1\n",
    "\n",
    "```\n",
    "Budget: 1/10, Spread: 8.00\n",
    "Budget: 2/10, Spread: 14.00\n",
    "Budget: 3/10, Spread: 18.00\n",
    "Budget: 4/10, Spread: 22.00\n",
    "Budget: 5/10, Spread: 25.00\n",
    "Budget: 6/10, Spread: 27.00\n",
    "Budget: 7/10, Spread: 29.00\n",
    "Budget: 8/10, Spread: 31.00\n",
    "Budget: 9/10, Spread: 33.00\n",
    "Budget: 10/10, Spread: 35.00\n",
    "['11-30471', '15-00571', '22-17024', '23-17065', '34-30758', '61-26427', '62-23911', '67-08444', '87-16214', '89-19116']\n",
    "```\n",
    "\n",
    "* B = 10, it = 5\n",
    " \n",
    "```\n",
    "Budget: 1/10, Spread: 8.00\n",
    "Budget: 2/10, Spread: 14.00\n",
    "Budget: 3/10, Spread: 18.00\n",
    "Budget: 4/10, Spread: 22.00\n",
    "Budget: 5/10, Spread: 25.00\n",
    "Budget: 6/10, Spread: 27.00\n",
    "Budget: 7/10, Spread: 29.00\n",
    "Budget: 8/10, Spread: 31.00\n",
    "Budget: 9/10, Spread: 33.00\n",
    "Budget: 10/10, Spread: 35.00\n",
    "['03-20962', '15-00571', '18-32052', '22-17024', '23-17065', '57-36745', '61-26427', '67-08444', '87-16214', '89-19116']\n",
    "```\n",
    " \n",
    " * B = 10, it = 10:\n",
    " \n",
    "```\n",
    "Budget: 1/10, Spread: 8.00\n",
    "Budget: 2/10, Spread: 14.00\n",
    "Budget: 3/10, Spread: 18.00\n",
    "Budget: 4/10, Spread: 22.00\n",
    "Budget: 5/10, Spread: 24.80\n",
    "Budget: 6/10, Spread: 27.00\n",
    "Budget: 7/10, Spread: 29.00\n",
    "Budget: 8/10, Spread: 31.00\n",
    "Budget: 9/10, Spread: 32.90\n",
    "Budget: 10/10, Spread: 35.00\n",
    "['08-09915', '09-38244', '22-17024', '23-17067', '34-30758', '61-26427', '67-08444', '78-09824', '87-16214', '91-14026']\n",
    "\n",
    "```\n",
    "\n",
    " * B = 10, it = 50\n",
    " \n",
    "```\n",
    "Budget: 1/10, Spread: 8.00\n",
    "Budget: 2/10, Spread: 13.98\n",
    "Budget: 3/10, Spread: 18.00\n",
    "Budget: 4/10, Spread: 21.98\n",
    "Budget: 5/10, Spread: 24.92\n",
    "Budget: 6/10, Spread: 26.90\n",
    "Budget: 7/10, Spread: 28.84\n",
    "Budget: 8/10, Spread: 30.94\n",
    "Budget: 9/10, Spread: 32.90\n",
    "Budget: 10/10, Spread: 34.80\n",
    "['11-30471', '25-25842', '34-30758', '61-21414', '63-26631', '74-02002', '78-09824', '80-16439', '89-19116', '89-33025']\n",
    "```\n",
    "\n",
    " * B = 10, it = 150\n",
    "```\n",
    "Budget: 1/10, Spread: 7.97\n",
    "Budget: 2/10, Spread: 13.98\n",
    "Budget: 3/10, Spread: 17.96\n",
    "Budget: 4/10, Spread: 21.97\n",
    "Budget: 5/10, Spread: 24.90\n",
    "Budget: 6/10, Spread: 26.86\n",
    "Budget: 7/10, Spread: 28.89\n",
    "Budget: 8/10, Spread: 30.89\n",
    "Budget: 9/10, Spread: 32.83\n",
    "Budget: 10/10, Spread: 34.85\n",
    "['09-38244', '18-32052', '22-17024', '34-30758', '48-27832', '61-26427', '64-25797', '74-02002', '87-16214', '89-19116']\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
